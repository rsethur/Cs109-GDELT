{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis of GDELT Dataset using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init() \n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawEvents = sc.textFile('gdelt/20151105.export.CSV').map(lambda x: x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event Count:  236093\n"
     ]
    }
   ],
   "source": [
    "#Schema info from: https://bigquery.cloud.google.com/table/gdelt-bq:full.events?pli=1\n",
    "eventsRDDWithDataTypes = rawEvents.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]), float(p[4]) , p[5], p[6], p[7], p[8], p[9], p[10],p[11],p[12], p[13], p[14], p[15], p[16],p[17],p[18],p[19],p[20],p[21],p[22],p[23],p[24],int(p[25]),p[26],p[27],p[28],int(p[29]),p[30],int(p[31]),int(p[32]),\n",
    "                               int(p[33]),float(p[34]),int(p[35]),p[36],p[37],p[38],p[39],\n",
    "                               p[40],p[41],int(p[42]),p[43],p[44],p[45],p[46],p[47],p[48],p[49],p[50],\n",
    "                               p[51],p[52],p[53],p[54],p[55],p[56],p[57]) ) #.toDF(schema)\n",
    "#note: some of the lat/long fields are null - so the bove code is not casting it into Float. In the final code we need to perform type conversion properly\n",
    "#eventsDF.cache()\n",
    "print \"Event Count: \", eventsRDDWithDataTypes.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schemaString =  [\"GLOBALEVENTID\", \"SQLDATE\", \"MonthYear\", \"Year\", \"FractionDate\", \"Actor1Code\", \"Actor1Name\", \"Actor1CountryCode\", \"Actor1KnownGroupCode\", \"Actor1EthnicCode\", \"Actor1Religion1Code\", \"Actor1Religion2Code\", \"Actor1Type1Code\", \"Actor1Type2Code\", \"Actor1Type3Code\", \"Actor2Code\", \"Actor2Name\", \"Actor2CountryCode\", \"Actor2KnownGroupCode\", \"Actor2EthnicCode\", \"Actor2Religion1Code\", \"Actor2Religion2Code\", \"Actor2Type1Code\", \"Actor2Type2Code\", \"Actor2Type3Code\", \"IsRootEvent\", \"EventCode\", \"EventBaseCode\", \"EventRootCode\", \"QuadClass\", \"GoldsteinScale\", \"NumMentions\", \"NumSources\", \"NumArticles\", \"AvgTone\", \"Actor1Geo_Type\", \"Actor1Geo_FullName\", \"Actor1Geo_CountryCode\", \"Actor1Geo_ADM1Code\", \"Actor1Geo_Lat\", \"Actor1Geo_Long\", \"Actor1Geo_FeatureID\", \"Actor2Geo_Type\", \"Actor2Geo_FullName\", \"Actor2Geo_CountryCode\", \"Actor2Geo_ADM1Code\", \"Actor2Geo_Lat\", \"Actor2Geo_Long\", \"Actor2Geo_FeatureID\", \"ActionGeo_Type\", \"ActionGeo_FullName\", \"ActionGeo_CountryCode\", \"ActionGeo_ADM1Code\", \"ActionGeo_Lat\", \"ActionGeo_Long\", \"ActionGeo_FeatureID\", \"DATEADDED\", \"SOURCEURL\"]\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString]\n",
    "fields[0].dataType =IntegerType()\n",
    "fields[1].dataType =IntegerType()\n",
    "fields[2].dataType =IntegerType()\n",
    "fields[3].dataType =IntegerType()\n",
    "fields[4].dataType =FloatType()\n",
    "fields[25].dataType =IntegerType()\n",
    "fields[29].dataType =IntegerType()\n",
    "fields[31].dataType =IntegerType()\n",
    "fields[32].dataType =IntegerType()\n",
    "fields[34].dataType =FloatType()\n",
    "fields[42].dataType =IntegerType()\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#pyspark.sql.types.DateConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "eventsDF = sqlContext.createDataFrame(eventsRDDWithDataTypes, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(GLOBALEVENTID=481938577, SQLDATE=20141105, MonthYear=201411, Year=2014, FractionDate=2014.8355712890625, Actor1Code=u'', Actor1Name=u'', Actor1CountryCode=u'', Actor1KnownGroupCode=u'', Actor1EthnicCode=u'', Actor1Religion1Code=u'', Actor1Religion2Code=u'', Actor1Type1Code=u'', Actor1Type2Code=u'', Actor1Type3Code=u'', Actor2Code=u'CHN', Actor2Name=u'BEIJING', Actor2CountryCode=u'CHN', Actor2KnownGroupCode=u'', Actor2EthnicCode=u'', Actor2Religion1Code=u'', Actor2Religion2Code=u'', Actor2Type1Code=u'', Actor2Type2Code=u'', Actor2Type3Code=u'', IsRootEvent=1, EventCode=u'040', EventBaseCode=u'040', EventRootCode=u'04', QuadClass=1, GoldsteinScale=u'1.0', NumMentions=26, NumSources=2, NumArticles=u'26', AvgTone=0.5453213453292847, Actor1Geo_Type=u'0', Actor1Geo_FullName=u'', Actor1Geo_CountryCode=u'', Actor1Geo_ADM1Code=u'', Actor1Geo_Lat=u'', Actor1Geo_Long=u'', Actor1Geo_FeatureID=u'', Actor2Geo_Type=4, Actor2Geo_FullName=u'Beijing, Beijing, China', Actor2Geo_CountryCode=u'CH', Actor2Geo_ADM1Code=u'CH22', Actor2Geo_Lat=u'39.9289', Actor2Geo_Long=u'116.388', Actor2Geo_FeatureID=u'-1898541', ActionGeo_Type=u'4', ActionGeo_FullName=u'Beijing, Beijing, China', ActionGeo_CountryCode=u'CH', ActionGeo_ADM1Code=u'CH22', ActionGeo_Lat=u'39.9289', ActionGeo_Long=u'116.388', ActionGeo_FeatureID=u'-1898541', DATEADDED=u'20151105', SOURCEURL=u'http://www.thestar.com.my/News/Regional/2015/11/05/Like-father-like-son-Hsien-Loong-replicates-Kuan-Yews-role-as-political-conduit/')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
